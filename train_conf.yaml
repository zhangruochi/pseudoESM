model:
  protein_bert_base:
    arch: protein_bert_base
    embed_dim: 768
    layers: 6
    ffn_embed_dim: 3072
    attention_heads: 12
    final_bias: True


train:
  random_seed: 7
  num_epoch: 5
  batch_size: 16
  num_workers: 16
  device_ids: [0,1,2,3]
  warmup_steps_ratio: 0.1
  weight_decay: 0.01
  learning_rate: 5e-5
  adam_epsilon: 1e-8
  gradient_accumulation_steps: 10
  max_grad_norm: 1
  eval_per_steps: 3000

data:
  total_train_num: 2000000
  total_valid_num: 50000
  total_test_num: 50000
  train_dir: "./data/train"
  valid_dir: "./data/eval"
  test_dir: "./data/test"

logger:
  log: True
  log_dir: "outputs"
  log_per_steps: 5
  final_artifact_path: final
  mlflow:
    AWS_ACCESS_KEY_ID: root
    AWS_SECRET_ACCESS_KEY: rootroot
    MLFLOW_S3_ENDPOINT_URL: http://192.168.1.232:6000
    MLFLOW_TRACKING_URI: http://192.168.1.232:7000
    MLFLOW_EXPERIMENT_NAME: "pseudo_esm"
    REGISTERED_MODEL_NAME: "pseudo_esm"


inference:
  model_path: "/data/zhangruochi/projects/pseudoESM/outputs/2022-07-19/22-42-51/model_step_1_f1_0.182"

other:
  debug: False
  debug_step: 5

