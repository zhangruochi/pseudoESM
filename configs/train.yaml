mode:
  gpu: true

model:
  protein_bert_base:
    arch: msa_transformer
    embed_dim: 768
    max_positions: 512
    layers: 12
    ffn_embed_dim: 3072
    attention_heads: 12
    final_bias: True
  task:
    max_length: 512
    mlm: true
    mlm_probability: 0.15

train:
  amp: true
  random_seed: 7
  num_epoch: 10
  batch_size: 6
  num_workers: 4
  weight_decay: 0
  lr_scheduler:
    T_0: 10000
    T_mult: 2
    eta_min: 5e-8
  learning_rate: 1e-4
  adam_epsilon: 1e-8
  gradient_accumulation_steps: 12
  max_grad_norm: 12
  eval_per_steps: 5000
  pin_memory: False
  device_ids: []

data:
  train_dir: "./data/train"
  valid_dir: "./data/eval"
  test_dir: "./data/test"

logger:
  log: True
  log_dir: "outputs"
  log_per_steps: 100
  final_artifact_path: final
  mlflow:
    AWS_ACCESS_KEY_ID: root
    AWS_SECRET_ACCESS_KEY: rootroot
    MLFLOW_S3_ENDPOINT_URL: http://192.168.1.232:6000
    MLFLOW_TRACKING_URI: http://192.168.1.232:6006
    MLFLOW_EXPERIMENT_NAME: "pseudo_esm"
    REGISTERED_MODEL_NAME: "pseudo_esm"


inference:
  test_dir:  "./data/test"
  device_ids: [0]
  batch_size: 32
  num_workers: 1
  model_path: "./pretrained_model/model_step_50_f1_0.223"

other:
  debug: True
  debug_step: 5

